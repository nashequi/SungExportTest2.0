# Load dependencies
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import urllib.request
import time
import bs4 as bs
import seaborn as sns
from scipy import stats

# To make pyproj conversions work
import os 
os.environ['PROJ_LIB']=r"C:\Users\Dean\Documents\Sung Test"

total_pages = 446
links = pd.DataFrame(columns=['link'])
links_done = 0


for page in range(1,(total_pages + 1)):
    
    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'
    url = "https://www.zoocasa.com/toronto-on-sold-listings?page=" + str(page)
    headers={'User-Agent':user_agent,} 

    request=urllib.request.Request(url,None,headers) #The assembled request
    data_raw = urllib.request.urlopen(request).read()
    data_split = data_raw.split(b'/listing-status>')[1:]
    time.sleep(1)
    
    for post in range(24): 

        try:
            start = data_split[post].find(b'href="/')
            end = data_split[post].find(b'-vow"')
            links.loc[links_done] = data_split[post][(start+7):(end+4)]

            links_done += 1
        except:
            print("Problem on Page" +str("page")+"!")
            
links.to_csv('house_links.csv')

# Prepare a table for data population
links = pd.read_csv('house_links.csv', index_col='Unnamed: 0')
data = pd.DataFrame(columns=['title','final_price', 'list_price','bedrooms','bathrooms','sqft','parking','description','mls','type','full_link','full_address', 'lat', 'long', 'city_district'])

start_time = time.time()
skipped = []
house_length = links.shape[0]

for no in range(house_length):
    try:
        url = 'https://www.zoocasa.com/' + str(links.link[no])[2:-1]

        #read data, convert to 2 formats
        request=urllib.request.Request(url,None,headers) #The assembled request
        data_raw = urllib.request.urlopen(request).read()
        soup = bs.BeautifulSoup(data_raw, 'lxml')

        #full link
        data.loc[no,'full_link'] = url

        #title
        data.loc[no,'title'] = soup.title.contents[0]

        #MLS
        mls_a = data_raw.find(b'Number</span>\n            <span>')
        mls_b = data_raw[(mls_a+32):].find(b'</span>')
        data.loc[no,'mls'] = data_raw[(mls_a+32):((mls_a+mls_b+32))]

        #Type
        type_a = data_raw.find(b'Type</span>\n            <span>')
        type_b = data_raw[(type_a+30):].find(b'</span>')
        data.loc[no,'type'] = data_raw[(type_a+30):((type_a+type_b+30))]

        #the rest
        table_html = soup.findAll("span", {"class": "blur"})
        for n,c in enumerate(['final_price', 'list_price','bedrooms','bathrooms','sqft','parking','description']):
            data.loc[no, c] = table_html[n].contents[0]

        #location data
        address = soup.title.contents[0]
        address = address.replace('Circ', 'Cir') #edit #1
        address_b = address.find('(')
        address_a = address.find('-')
        
        data.loc[no,'full_address'] = address[(address_a+1):address_b]

#         location = geolocator.geocode(address[(address_a+1):address_b], timeout = 3, addressdetails=True)
#         data.loc[no,'full_address'] = location.address
#         data.loc[no,'lat'] = location.latitude
#         data.loc[no,'long'] = location.longitude
#         try: data.loc[no,'city_district'] = location.raw['address']['city_district']
#         except:data.loc[no,'city_district'] = location.raw['address']['city']


        #pause and print
        time.sleep(1)
        if no % 10 == 0: print(no, 'is done')
    
    except: 
        print('Problem with: ', no)
        skipped.append(no)
        continue
    
print('Skipped: ', skipped, ', Total: ', len(skipped))
print('Total time: %.2f' % (time.time() - start_time))

data.to_excel('data.xlsx')

houses = pd.read_excel('data.xlsx', index_col=0)

# Adding province name and country to be precise
houses[['full_address']] = houses[['full_address']].astype(str) + ', Ontario, Canada'
houses.head(2)
